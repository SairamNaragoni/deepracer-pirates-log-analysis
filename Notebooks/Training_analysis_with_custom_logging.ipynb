{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training analysis for DeepRacer\n",
    "\n",
    "This notebook has been built based on the `DeepRacer Log Analysis.ipynb` provided by the AWS DeepRacer Team. It has been reorganised and expanded to provide new views on the training data without the helper code which was moved into utility `.py` files.\n",
    "\n",
    "## Usage\n",
    "\n",
    "I have expanded this notebook from to present how I'm using this information. It contains descriptions that you may find not that needed after initial reading. Since this file can change in the future, I recommend that you make its copy and reorganize it to your liking. This way you will not lose your changes and you'll be able to add things as you please.\n",
    "\n",
    "**This notebook isn't complete.** What I find interesting in the logs may not be what you will find interesting and useful. I recommend you get familiar with the tools and try hacking around to get the insights that suit your needs.\n",
    "\n",
    "## Contributions\n",
    "\n",
    "As usual, your ideas are very welcome and encouraged so if you have any suggestions either bring them to [the AWS DeepRacer Community](http://join.deepracing.io) or share as code contributions.\n",
    "\n",
    "## Training environments\n",
    "\n",
    "Depending on whether you're running your training through the console or using the local setup, and on which setup for local training you're using, your experience will vary. As much as I would like everything to be taylored to your configuration, there may be some problems that you may face. If so, please get in touch through [the AWS DeepRacer Community](http://join.deepracing.io).\n",
    "\n",
    "## Requirements\n",
    "\n",
    "Before you start using the notebook, you will need to install some dependencies. If you haven't yet done so, have a look at [The README.md file](/edit/README.md#running-the-notebooks) to find what you need to install.\n",
    "\n",
    "Apart from the install, you also have to configure your programmatic access to AWS. Have a look at the guides below, AWS resources will lead you by the hand:\n",
    "\n",
    "AWS CLI: https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html\n",
    "\n",
    "Boto Configuration: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html\n",
    "\n",
    "## Credits\n",
    "\n",
    "I would like to thank [the AWS DeepRacer Community](http://join.deepracing.io) for all the feedback about the notebooks. If you'd like, follow [my blog](https://codelikeamother.uk) where I tend to write about my experiences with AWS DeepRacer.\n",
    "\n",
    "# Log Analysis\n",
    "\n",
    "Let's get to it.\n",
    "\n",
    "## Imports\n",
    "\n",
    "Run the imports block below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import os.path\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#Shapely Library\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely.geometry.polygon import LinearRing, LineString\n",
    "\n",
    "#Plotly Library\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from python import track_utils as tu\n",
    "from python import cw_utils as cw\n",
    "from python import log_analysis as la\n",
    "from python import plotly_graph_utils as pg\n",
    "from python import pirates_log_analysis as pla\n",
    "\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "# Make sure your boto version is >= '1.9.133'\n",
    "cw.boto3.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block below has been prepared in case you would like to make some changes to the utility code that comes with this notebook. It will reload track_utlis.py, log_analysis.py and cw_utils.py without the need to reload the notebook. In normal usage of the notebook you will not need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload log_analysis and the rest of stuff here if needed\n",
    "# (handy for code updates in utils, doesn't break anything if no changes)\n",
    "import importlib\n",
    "importlib.reload(la)\n",
    "importlib.reload(cw)\n",
    "importlib.reload(tu)\n",
    "importlib.reload(pg)\n",
    "importlib.reload(pla)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load waypoints for the track you want to run analysis on\n",
    "\n",
    "The track waypoint files usually show up as new races start. Be sure to check for them in repository updates. You only need to load them in the block below.\n",
    "\n",
    "These files represent the coordinates of characteristic points of the track - the center line, inside border and outside border. Their main purpose is to visualise the track in images below. One thing that you may want to remember is that at the moment not all functions below work with all values of the coordinates. Especially some look awkward with bigger tracks or with negative coordinates. Usually there is an explanation on what to do to fix the view.\n",
    "\n",
    "The naming of the tracks is not super consistent. I'm also not sure all of them are available in the console or locally. You may want to know that:\n",
    "* London_Loop and Virtual_May19_Train_track - are the AWS DeepRacer Virtual League London Loop tracks\n",
    "* Tokyo - is the AWS DeepRacer Virtual League Kumo Torakku track\n",
    "* New_York - are the AWS DeepRacer Virtual League Empire City training and evaluation tracks\n",
    "* China - are the AWS Deepracer Virtual League Shanghai Sudu training and evaluation tracks\n",
    "* reinvent_base - is the re:Invent 2019 racing track\n",
    "\n",
    "There are also other tracks that you may want to explore. Each of them has its own properties that you might find useful for your model.\n",
    "\n",
    "Remeber that evaluation npy files are a community effort to visualise the tracks in the trainings, they aren't 100% accurate.\n",
    "\n",
    "Tracks Available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conveniently list available tracks to analyze\n",
    "available_track_files = glob.glob(\"../Tracks/**.npy\")\n",
    "available_track_names = list(map(lambda x: os.path.basename(x).split('.npy')[0], available_track_files))\n",
    "available_track_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "TRACK_NAME = \"seaside\"\n",
    "log_dir = \"demo-datatypes-clone\"\n",
    "\n",
    "#Profile for downloading logs from CloudWatch\n",
    "PROFILE = \"adfs\"\n",
    "\n",
    "#DRFC number of workers for console training set this to 1\n",
    "workers = 1\n",
    "\n",
    "#Custom Logger Config \n",
    "custom_config = {}\n",
    "custom_config['custom_config'] =True\n",
    "custom_config['logger_prefix'] = 'PIRATES_TRACE_LOG:'\n",
    "custom_config['custom_headers'] = ['botx','boty','objects_location','objects_distance','reward_avoid']\n",
    "custom_config['array_headers'] = ['objects_location','objects_distance']\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_center_line, l_inner_border, l_outer_border, road_poly = tu.load_track(TRACK_NAME, \"../\")\n",
    "road_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotly plot configuration\n",
    "plotly_config = {}\n",
    "plotly_config[\"object_avoidance\"]=True\n",
    "plotly_config[\"height\"]=600 #Track scaling height\n",
    "plotly_config[\"width\"]=900 #Track scaling width\n",
    "plotly_config[\"track_name\"]=TRACK_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the logs\n",
    "\n",
    "Depending on which way you are training your model, you will need a different way to load the data.\n",
    "\n",
    "**DRFC With Multiple Workers**\n",
    "Specify the number of workers and append the log streams from cloudwatch\n",
    "\n",
    "**AWS DeepRacer Console**\n",
    "Download the logs from the Deepracer Console to the Logs folder. Give the name of the directory for log which you want to parse in the variable `log_dir`.\n",
    "\n",
    "**DeepRacer for Dummies/ARCC local training**\n",
    "Those two setups come with a container that runs Jupyter Notebook (as you noticed if you're using one of them and reading this text). Logs are stored in `/logs/` and you just need to point at the latest file to see the current training. The logs are split for long running training if they exceed 500 MB. The log loading method has been extended to support that.\n",
    "\n",
    "**Chris Rhodes' repo**\n",
    "Chris repo doesn't come with logs storage out of the box. I would normally run `docker logs dr > /path/to/logfile` and then load the file.\n",
    "\n",
    "Below I have prepared a section for each case. In each case you can analyse the logs as the training is being run, just in case of the Console you may need to force downloading of the logs as the `cw.download_log` method has a protection against needless downloads.\n",
    "\n",
    "Select your preferred way to get the logs below and you can get rid of the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_logs(robomaker_stream_name):\n",
    "    robomakers=[]\n",
    "    for i in range(len(robomaker_stream_name)):\n",
    "        robomaker_fname = \"robomaker-\"+str(i+1)+\".log\"\n",
    "        robomaker_fname = log_path+robomaker_fname\n",
    "        robomakers.append(robomaker_fname)\n",
    "        cw.download_log(fname=robomaker_fname, stream_name=robomaker_stream_name[i], log_group=LOG_GROUP, force=True, profile=PROFILE)\n",
    "    return robomakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Deepracer for Cloud\n",
    "# LOG_GROUP = \"/deepracer-for-cloud\"\n",
    "# log_path = \"../Logs/\"+log_dir+\"/logs/training/\"\n",
    "# Path(log_path).mkdir(parents=True, exist_ok=True)\n",
    "# sagemaker_fname = log_dir + \"-drfc-sagemaker.log\"\n",
    "# sagemaker_fname = log_path + sagemaker_fname\n",
    "\n",
    "# robomaker_stream_name=[]\n",
    "\n",
    "# #Append depending on number of workers.\n",
    "# robomaker_stream_name.append(\"stream-name1\")\n",
    "# robomaker_stream_name.append(\"stream-name2\")\n",
    "# robomaker_stream_name.append(\"stream-name3\")\n",
    "# sagemaker_stream_name = \"deepracer-0_rl_coach.1.1jovv09fgns6asky9ivn0lhn2\"\n",
    "\n",
    "# robomakers = download_logs(robomaker_stream_name)\n",
    "# robomaker_fname = robomakers[0]\n",
    "# cw.download_log(fname=sagemaker_fname, stream_name=sagemaker_stream_name, log_group=LOG_GROUP, force=True, profile=PROFILE)\n",
    "# (robomakers,sagemaker_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS DeepRacer Console (Comment this if you are using DRFC)\n",
    "sagemaker_fname =  glob.glob(\"../Logs/\"+log_dir+\"/logs/training/**sagemaker.log\")[0]\n",
    "robomaker_fname = glob.glob(\"../Logs/\"+log_dir+\"/logs/training/**robomaker.log\")[0]\n",
    "robomakers = [robomaker_fname]\n",
    "sagemaker_fname,robomaker_fname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the trace training log\n",
    "\n",
    "Now that the data is downloaded, we need to load it into memory. We will first read it from file and then convert to data frames in Pandas. [Pandas](https://pandas.pydata.org/) is a Python library for handling and analysing large amounts of data series. Remember this name, you may want to learn more about how to use it to get more information that you would like to get from the logs. Examples below are hardly scratching the surface.\n",
    "\n",
    "One important information to enter is the setting of your Episodes per iteration hyperparameter. This is used to group the episodes into iterations. This information is valuable when later looking at graphs showing how the training progresses per iteration. You can use it to detect which iteration gave you better outcomes and, if in local training, you could move to that iteration's outcome for submissions in the AWS DeepRacer League or  for continuing the training.\n",
    "\n",
    "The log files you have just gathered above have lines like this one:\n",
    "```\n",
    "SIM_TRACE_LOG:799,111,1.7594,4.4353,3.0875,-0.26,2.50,2,1.0000,False,True,71.5802,49,17.67,1555554451.1110387\n",
    "```\n",
    "This is all that matters for us. The first two are some tests I believe and when loading they get skipped, then each next line has the following fields:\n",
    "* episode number\n",
    "* step number\n",
    "* x coordinate\n",
    "* y coordinate\n",
    "* yaw of the car (where the car is heading)\n",
    "* decision about turning (turn value from your action space)\n",
    "* decision about throttle (speed value from your action space)\n",
    "* decision index (value from your action space)\n",
    "* reward value\n",
    "* is the car going backwards\n",
    "* are all wheels on track?\n",
    "* progress in the lap\n",
    "* closest waypoint\n",
    "* track length\n",
    "* timestamp\n",
    "\n",
    "`la.load_data` and then `la.convert_to_pandas` read it and prepare for your usage. Sorting the values may not be needed, but I have experienced under some circumstances that the log lines were not ordered properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_robomaker_logs_hyperparameters(robomaker_fname, hyperparameter_name):\n",
    "    hyperparameter_name = hyperparameter_name.replace('_', '[_]')\n",
    "    os_cmd = \"cat %s | grep -E '^[ ]+[\\\"]%s[\\\"][:]' | cut -d':' -f2 | cut -d',' -f1 | sed -e 's/^[ ]*//g'\" % (robomaker_fname, hyperparameter_name)\n",
    "    os_output = !$os_cmd\n",
    "    return os_output[0]\n",
    "\n",
    "# use this manually in windows\n",
    "# EPISODES_PER_ITERATION = 40\n",
    "\n",
    "EPISODES_PER_ITERATION = int(parse_robomaker_logs_hyperparameters(robomaker_fname, 'num_episodes_between_training'))\n",
    "NUM_EPOCHS = int(parse_robomaker_logs_hyperparameters(robomaker_fname, 'num_epochs'))\n",
    "\n",
    "print ('EPISODES_PER_ITERATION: %s' % EPISODES_PER_ITERATION)\n",
    "EPISODES_PER_ITERATION = int(EPISODES_PER_ITERATION/(workers))\n",
    "print ('EPISODES_PER_ITERATION per robomaker: %s' % EPISODES_PER_ITERATION)\n",
    "print ('NUM_EPOCHS: %s' % NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Logs\n",
    "cst = (workers-1)*EPISODES_PER_ITERATION\n",
    "for w in range(workers):\n",
    "    data = pla.load_data(robomakers[w],custom_config)\n",
    "    wdf = pla.convert_to_pandas(data,custom_config, episodes_per_iteration=EPISODES_PER_ITERATION)\n",
    "    wdf = wdf.sort_values(['episode', 'steps'])\n",
    "    wdf['episode'] += (wdf['iteration']-1)*cst+w*EPISODES_PER_ITERATION\n",
    "    if w == 0:\n",
    "        df = wdf.copy()\n",
    "    else :        \n",
    "        df = pd.concat([df, wdf], ignore_index=True)\n",
    "df = df.sort_values(['episode', 'steps'])\n",
    "\n",
    "# personally I think normalizing can mask too high rewards so I am commenting it out,\n",
    "# but you might want it.\n",
    "# la.normalize_rewards(df)\n",
    "\n",
    "#Uncomment the line of code below to evaluate a different reward function\n",
    "# la.new_reward(df, l_center_line, 'reward.new_reward') #, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to cut down the training till the specified checkpoint episode\n",
    "Ex : If Best Checkpoint is at episode = 3250, cut down training data until that episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_ep = 3250\n",
    "# ep = df[df['episode']==checkpoint_ep]\n",
    "# step_no = int(ep.iloc[[-1]].index.tolist()[0])\n",
    "# df = df.iloc[0:step_no]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New reward\n",
    "\n",
    "Note the last line above: it takes a reward class from log-analysis/rewards, imports it, instantiates and recalculates reward values based on the data from the log. This lets you do some testing before you start training and rule out some obvious things.\n",
    "\n",
    "*If you find this confusing, don't worry, because it is confusing. You can safely ignore it for now and come back to it later.*\n",
    "\n",
    "This operation is possible because the logs contain all information needed to recreate the params for a given step. That said some could be implemented better and some were ignored for now and should be implemented.\n",
    "\n",
    "The sample reward mentioned in that line is located in `log-analysis/rewards/reward_sample.py` and looks like this:\n",
    "\n",
    "```\n",
    "from time import time\n",
    "\n",
    "\n",
    "class Reward:\n",
    "    def __init__(self, verbose=False):\n",
    "        self.previous_steps = None\n",
    "        self.initial_time = None\n",
    "        self.verbose = verbose\n",
    "\n",
    "    @staticmethod\n",
    "    def get_time(params):\n",
    "        # remember: this will not return time before\n",
    "        # the first step has completed so the total\n",
    "        # time for lap will be lower by about 0.2s\n",
    "        return params.get('timestamp', None) or time()\n",
    "\n",
    "    def reward_function(self, params):\n",
    "        if self.previous_steps is None \\\n",
    "                or self.previous_steps > params['steps']:\n",
    "            # new lap!\n",
    "            self.initial_time = self.get_time(params)\n",
    "        else:\n",
    "            # we're continuing a lap\n",
    "            pass\n",
    "\n",
    "        steering_factor = 1.0\n",
    "\n",
    "        if abs(params['steering_angle']) > 14:\n",
    "            steering_factor = 0.7\n",
    "\n",
    "        reward = float(steering_factor)\n",
    "\n",
    "        self.previous_steps = params['steps']\n",
    "\n",
    "        if self.verbose:\n",
    "            print(params)\n",
    "\n",
    "        return reward\n",
    "\n",
    "\n",
    "reward_object = Reward()\n",
    "\n",
    "\n",
    "def reward_function(params):\n",
    "    return reward_object.reward_function(params)\n",
    "\n",
    "```\n",
    "\n",
    "After some imports a class is declared, it's called `Reward`, then the class is instantiated and a function `reward_function` is declared. This somewhat bloated structure has a couple benefits:\n",
    "* It works in console/local training for actual training\n",
    "* It lets you reload the definition for class Reward and retry the reward function multiple times after changes without much effort\n",
    "* If you want to rely on state carried over between the steps, it's all contained in a reward object \n",
    "\n",
    "The reward class hides two or three tricks for you:\n",
    "* `get_time` lets you abstract from machine time in log analysis - the supporting code adds one extra param, `timestamp`. That lets you get the right time value in new_reward function\n",
    "* the first condition allows detecting the beginning of an episode or even start of training you can use it for some extra operations between the episodes\n",
    "* `verbose` can be used to provide some noisier prints in the reward function - you can switch them on when loading the reward function above.\n",
    "\n",
    "Just remember: not all params are provided, you are free to implement them and raise a Pull Request for log_analysis.df_to_params method.\n",
    "\n",
    "If you just wrap your reward function like in the above example, you can use it in both log analysis notebook and the training.\n",
    "\n",
    "Final warning: there is a loss of precision in the logs (rounded numbers) and also potentially potential bugs. If you find any, please fix, please report.\n",
    "\n",
    "## Graphs\n",
    "\n",
    "The original notebook has provided some great ideas on what could be visualised in the graphs. Below examples are a slightly extended version. Let's have a look at what they are presenting and what this may mean to your training.\n",
    "\n",
    "### Training progress\n",
    "\n",
    "As you have possibly noticed by now, training episodes are grouped into iterations and this notebook also reflects it. What also marks it are checkpoints in the training. After each iteration a set of ckpt files is generated - they contain outcomes of the training, then a model.pb file is built based on that and the car begins a new iteration. Looking at the data grouped by iterations may lead you to a conclusion, that some earlier checkpoint would be a better start for a new training. While this is limited in the AWS DeepRacer Console, with enough disk space you can keep all the checkpoints along the way and use one of them as a start for new training (or even as a submission to a race).\n",
    "\n",
    "While the episodes in a given iteration are a mixture of decision process and random guesses, mean results per iteration may show a specific trend. Mean values are accompanied by standard deviation to show the concentration of values around the mean.\n",
    "\n",
    "#### Rewards per Iteration\n",
    "\n",
    "You can see these values as lines or dots per episode in the AWS DeepRacer console. When the reward goes up, this suggests that a car is learning and improving with regards to a given reward function. **This does not have to be a good thing.** If your reward function rewards something that harms performance, your car will learn to drive in a way that will make results worse.\n",
    "\n",
    "At first the rewards just grow if the progress achieved grows. Interesting things may happen slightly later in the training:\n",
    "\n",
    "* The reward may go flat at some level - it might mean that the car can't get any better. If you think you could still squeeze something better out of it, review the car's progress and consider updating the reward function, the action space, maybe hyperparameters, or perhaps starting over (either from scratch or from some previous checkpoint)\n",
    "* The reward may become wobbly - here you will see it as a mesh of dots zig-zagging. It can be a gradually growing zig-zag or a roughly stagnated one. This usually means the learning rate hyperparameter is too high and the car started doing actions that oscilate around some local extreme. You can lower the learning rate and hope to step closer to the extreme. Or run away from it if you don't like it\n",
    "* The reward plunges to near zero and stays roughly flat - I only had that when I messed up the hyperparameters or the reward function. Review recent changes and start training over or consider starting from scratch\n",
    "\n",
    "The Standard deviation says how close from each other the reward values per episode in a given iteration are. If your model becomes reasonably stable and worst performances become better, at some point the standard deviation may flat out or even decrease. That said, higher speeds usually mean there will be areas on track with higher risk of failure. This may bring the value of standard deviation to a higher value and regardless of whether you like it or not, you need to accept it as a part of fighting for significantly better times.\n",
    "\n",
    "#### Time per iteration\n",
    "\n",
    "I'm not sure how useful this graph is. I would worry if it looked very similar to the reward graph - this could suggest that slower laps will be getting higher rewards. But there is a better graph for spotting that below.\n",
    "\n",
    "#### Progress per Iteration\n",
    "\n",
    "This graph usually starts low and grows and at some point it will get flatter. The maximum value for progress is 100% so it cannot grow without limits. It usually shows similar initial behaviours to reward and time graphs. I usually look at it when I alter an action in training. In such cases this graph usually dips a bit and then returns or goes higher.\n",
    "\n",
    "#### Total reward per episode\n",
    "\n",
    "This graph has been taken from the orignal notebook and can show progress on certain groups of behaviours. It usually forms something like a triangle, sometimes you can see a clear line of progress that shows some new way has been first taught and then perfected.\n",
    "\n",
    "#### Mean completed lap times per iteration\n",
    "\n",
    "Once we have a model that completes laps reasonably often, we might want to know how fast the car gets around the track. This graph will show you that. I use it quite often when looking for a model to shave a couple more miliseconds. That said it has to go in pair with the last one:\n",
    "\n",
    "#### Completion rate per iteration\n",
    "\n",
    "It represents how big part of all episodes in an iteration is full laps. The value is from range [0, 1] and is a result of deviding amount of full laps in iteration by amount of all episodes in iteration. I say it has to go in pair with the previous one because you not only need a fast lapper, you also want a race completer.\n",
    "\n",
    "The higher the value, the more stable the model is on a given track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats for all laps\n",
    "\n",
    "Previous graphs were mainly focused on the state of training with regards to training progress. This however will not give you a lot of information about how well your reward function is doing overall.\n",
    "\n",
    "In such case `scatter_aggregates` may come handy. It comes with three types of graphs:\n",
    "* progress/steps/reward depending on the time of an episode - of this I find reward/time and new_reward/time especially useful to see that I am rewarding good behaviours - I expect the reward to time scatter to look roughly triangular\n",
    "* histograms of time and progress - for all episodes the progress one is usually quite handy to get an idea of model's stability\n",
    "* progress/time_if_complete/reward to closest waypoint at start - these are really useful during training as they show potentially problematic spots on track. It can turn out that a car gets best reward (and performance) starting at a point that just cannot be reached if the car starts elsewhere, or that there is a section of a track that the car struggles to get past and perhaps it's caused by an aggressive action space or undesirable behaviour prior to that place\n",
    "\n",
    "Side note: `time_if_complete` is not very accurate and will almost always look better for episodes closer to 100% progress than in case of those 50% and below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "simulation_agg = la.simulation_agg(df)\n",
    "pg.plot_progress_reward_distribution(simulation_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la.analyze_training_progress(simulation_agg, title='Training progress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "la.scatter_aggregates(simulation_agg, 'Stats for all laps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats for complete laps\n",
    "The graphs here are same as above, but now I am interested in other type of information:\n",
    "* does the reward scatter show higher rewards for lower completion times? If I give higher reward for a slower lap it might suggest that I am training the car to go slow\n",
    "* what does the time histogram look like? With enough samples available the histogram takes a normal distribution graph shape. The lower the mean value, the better the chance to complete a fast lap consistently. The longer the tails, the greater the chance of getting lucky in submissions\n",
    "* is the car completing laps around the place where the race lap starts? Or does it only succeed if it starts in a place different to the racing one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "complete_ones = simulation_agg[simulation_agg['progress']==100]\n",
    "print(len(complete_ones))\n",
    "if complete_ones.shape[0] > 0:\n",
    "    la.scatter_aggregates(complete_ones, 'Stats for complete laps')\n",
    "else:\n",
    "    print('No complete laps yet.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.plot_time_hist(complete_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.plot_complete_lap_analysis(complete_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.plot_distribution(complete_ones,column=\"steps\",percent=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.plot_distribution(complete_ones,column=\"time\",percent=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.plot_reward_distribution(df,percent=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.plot_reward_hist(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve policy training data from the SageMaker Log file\n",
    "trn_data = la.parse_sagemaker_logs(sagemaker_fname)\n",
    "pg.plot_training_metrics(trn_data)\n",
    "pg.plot_training_metrics(trn_data, \"surrogate_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories analysis\n",
    "We're going back to comparing training results based on the training time, but in a different way. Instead of just scattering things in relation to iteration or episode number, this time we're grouping episodes based on a certaing information. For this we use function:\n",
    "```\n",
    "analyze_categories(panda, category='quintile', groupcount=5, title=None)\n",
    "```\n",
    "The idea is pretty simple - determine a way to cluster the data and provide that as the `category` parameter (alongside the count of groups available). In the default case we take advantage of the aggregated information to which quintile an episode belongs and thus build buckets each containing 20% of episodes which happened around the same time during the training. If your training lasted for five hours, this would show results grouped per each hour.\n",
    "\n",
    "A side note: if you run the function with `category='start_at'` and `groupcount=20` you will get results based on the waypoint closest to the starting point of an episode. If you need to, you can introduce other types of categories and reuse the function.\n",
    "\n",
    "The graphs are similar to what we've seen above. I especially like the progress one which shows where the model tends to struggle and whether it's successful laps rate is improving or beginning to decrease. Interestingly, I also had cases where I saw the completion drop on the progress rate only to improve in a later quintile, but with a better time graph.\n",
    "\n",
    "A second side note: if you run this function for `complete_ones` instead of `simulation_agg`, suddenly the time histogram becomes more interesting as you can see whether completion times improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "la.analyze_categories(simulation_agg, title='Quintiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Percentage completion in each quintile\n",
    "quintiles = simulation_agg[\"quintile\"].unique().to_list();\n",
    "q_list = []\n",
    "for q in quintiles:\n",
    "    total_num_ep = simulation_agg[simulation_agg[\"quintile\"]==q].count()[\"quintile\"]\n",
    "    completed_num_ep = complete_ones[complete_ones[\"quintile\"]==q].count()[\"quintile\"]\n",
    "    q_list.append({\"quintile\": q, \"completed_ep\": completed_num_ep, \"total_ep\": total_num_ep, \"percent\": round(completed_num_ep/total_num_ep * 100, 2)})\n",
    "q_data = pd.DataFrame(q_list)\n",
    "q_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data in tables\n",
    "\n",
    "While a lot can be seen in graphs that cannot be seen in the raw numbers, the numbers let us get into more detail. Below you will find a couple examples. If your model is behaving the way you would like it to, below tables may provide little added value, but if you struggle to improve your car's performance, they may come handy. In such cases I look for examples where high reward is giving to below-expected episode and when good episodes are given low reward.\n",
    "\n",
    "You can then take the episode number and scatter it below, and also look at reward given per step - this can in turn draw your attention to some rewarding anomalies and help you detect some unexpected outcomes in your reward function.\n",
    "\n",
    "There is a number of ways to select the data for display:\n",
    "* `nlargest`/`nsmallest` lets you display information based on a specific value being highest or lowest\n",
    "* filtering based on a field value, for instance `df[df['episode']==10]` will display only those steps in `df` which belong to episode 10\n",
    "* `head()` lets you peek into a dataframe\n",
    "\n",
    "There isn't a right set of tables to display here and the ones below may not suit your needs. Get to know Pandas more and have fun with them. It's almost as addictive as DeepRacer itself.\n",
    "\n",
    "The examples have a short comment next to them explaining what they are showing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# View ten best rewarded episodes in the training\n",
    "simulation_agg.nlargest(100, 'new_reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## View five fastest complete laps\n",
    "complete_ones.nsmallest(50, 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# View five best rewarded completed laps\n",
    "complete_ones.nlargest(50, 'reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# View five best rewarded in completed laps (according to new_reward if you are using it)\n",
    "complete_ones.nlargest(50, 'new_reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "complete_ones[complete_ones['start_at']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# View all steps data for episode 10\n",
    "df[df['episode']==10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual episode/s plot\n",
    "Get the list of episodes that you want to analyse , probably from different quintiles to see the change in path that the episode takes and analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = [7,20]\n",
    "for episode_no in episodes:\n",
    "    episode_data = df[df['episode']==episode_no]\n",
    "    pg.plot_episode(episode_data,plotly_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Multiple Laps\n",
    "Use the below function to plot laps between specified times. This is useful to analyse the laps which are incomplete between specified start and end time and also to see the distribution for complete laps. <br>\n",
    "hover data : [reward , speed] , color = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pg.plot_multiple_laps(df,complete_ones,plotly_config,time_start = 7.8,time_end = 8.0,is_complete = True)\n",
    "pg.plot_multiple_laps(df,simulation_agg,plotly_config,time_start = 7.8,time_end = 8.0,is_complete = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration Plot\n",
    "Plot the path taken by the agent in multiple Iterations in a single graph with waypoints customization  \n",
    "`is_complete = True` # plots only complete_laps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = [1]\n",
    "starting_waypoint = 0\n",
    "ending_waypoint =330\n",
    "pg.plot_iterations(df,iterations,plotly_config,EPISODES_PER_ITERATION,starting_waypoint,ending_waypoint,is_complete=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path taken for top reward iterations\n",
    "\n",
    "NOTE: at some point in a single episode the car could go around multiple laps, the episode was terminated when car completed 1000 steps. Currently one episode has at most one lap. This explains why you can see multiple laps in an episode plotted below.\n",
    "\n",
    "Being able to plot the car's route in an episode can help you detect certain patterns in its behaviours and either promote them more or train away from them. While being able to watch the car go in the training gives some information, being able to reproduce it after the training is much more practical.\n",
    "\n",
    "Graphs below give you a chance to look deeper into your car's behaviour on track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Path taken by top 5 fastest laps\n",
    "# sorted_episodes = list(complete_ones.nsmallest(5,'time')['episode'])\n",
    "# sorted_episodes = list(complete_ones.nlargest(3,'reward')['episode'])\n",
    "# sorted_episodes = list(complete_ones[complete_ones['start_at']==0].nsmallest(5,'time')['episode'])\n",
    "sorted_episodes = list(simulation_agg.nlargest(5,'progress')['episode'])\n",
    "for episode in sorted_episodes:\n",
    "    episode_data = df[df['episode']==episode]\n",
    "    pg.plot_episode(episode_data,plotly_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a heatmap of rewards for current training. \n",
    "The brighter the colour, the higher the reward granted in given coordinates.\n",
    "If instead of a similar view as in the example below you get a dark image with hardly any \n",
    "dots, it might be that your rewards are highly disproportionate and possibly sparse.\n",
    "\n",
    "Disproportion means you may have one reward of 10.000 and the rest in range 0.01-1.\n",
    "In such cases the vast majority of dots will simply be very dark and the only bright dot\n",
    "might be in a place difficult to spot. I recommend you go back to the tables and show highest\n",
    "and average rewards per step to confirm if this is the case. Such disproportions may\n",
    "not affect your traning very negatively, but they will make the data less readable in this notebook.\n",
    "\n",
    "Sparse data means that the car gets a high reward for the best behaviour and very low reward\n",
    "for anything else, and worse even, reward is pretty much discrete (return 10 for narrow perfect,\n",
    "else return 0.1). The car relies on reward varying between behaviours to find gradients that can\n",
    "lead to improvement. If that is missing, the model will struggle to improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some tracks have negative coordinates for which `la.plot_track` is not ready. It has been improved\n",
    "# using RichardFan's modification to offer an x_shift and y_shift parameters. They may require\n",
    "# different values for other tracks. You will then need to change it in the future. Simply add parameters:\n",
    "# track_size=(700,1000), y_shift=300 to this method\n",
    "track = la.plot_track(df, l_center_line, l_inner_border, l_outer_border,track_size=(1500,2020),x_shift=1000,y_shift=700)\n",
    "plt.title(\"Reward distribution for all actions \")\n",
    "im = plt.imshow(track, cmap='hot', interpolation='bilinear', origin=\"lower\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a particular iteration\n",
    "This is same as the heatmap above, but just for a single iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_id = 10\n",
    "track = la.plot_track(df[df['iteration'] == iteration_id], l_center_line, l_inner_border, l_outer_border)\n",
    "plt.title(\"Reward distribution for all actions \")\n",
    "im = plt.imshow(track, cmap='hot', interpolation='bilinear', origin=\"lower\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action breakdown per iteration and historgram for action distribution for each of the turns - reinvent track\n",
    "\n",
    "This plot is useful to understand the actions that the model takes for any given iteration. Unfortunately at this time it is not fit for purpose as it assumes six actions in the action space and has other issues. It will require some work to get it to done but the information it returns will be very valuable.\n",
    "\n",
    "This is a bit of an attempt to abstract away from the brilliant function in the original notebook towards a more general graph that we could use. It should be treated as a work in progress. The track_breakdown could be used as a starting point for a general track information object to handle all the customisations needed in methods of this notebook.\n",
    "\n",
    "A breakdown track data needs to be available for it. If you cannot find it for the desired track, MAKEIT.\n",
    "\n",
    "Currently supported tracks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la.track_breakdown.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second parameter is either a single index or a list of indices for df iterations that you would like to view. You can for instance use `sorted_idx` list which is a sorted list of iterations from the highest to lowest reward.\n",
    "\n",
    "Bear in mind that you will have to provide a proper action naming in parameter `action_names`, this function assumes only six actions by default. I think they need to match numbering of actions in your model's metadata json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "la.action_breakdown(df, 4,df['iteration'], la.track_breakdown['reinvent2018'], l_center_line, l_inner_border, l_outer_border, ['0', '1', '2', '3',\n",
    "                                   '4', '5', '6', '7', \n",
    "                                   '8', '9', '10', '11', \n",
    "                                   '12', '13', '14', '15',\n",
    "                                   '16', '17', '18', '19', '20'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
